{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a tutorial for Q-learning using a very simple but comprehensive numerical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cells.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a robot is moving from C to F. The robot can only move in four directions: up, down, left and right. In the graph, shaded areas are obstacles that the robot cannot move to. This map could be represented as a graph where each cell is a node and linked by an edge. The arrow represents that the robot can move from one node to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graph.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reward value is set to edge from node to node. For example, if the link leads to the goal node F, the reward is set to 100, and others are all set to 0. If there is no links between two nodes, the reward are set to -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphReward.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q-learning, the aim is to reach the goal node with the highest reward. If the robot reached the goal, it has the highest reward so that it will remain there forever.\n",
    "\n",
    "Suppose the robot can learn from exprerience, but has no knowledge of the environment when passing from one node to another. It does not know which sequence of nodes will lead to the goal.\n",
    "\n",
    "In Q-learning and also reinforcement learning, the \"state\" represents a node and the \"action\" represents the robot's movement from one node to another. So in the graph, \"state\" is depicted as a node, while \"action\" is depicted by arrows.\n",
    "\n",
    "The reward values can be represented as the following reward matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rewardMatrix.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, robot does not see this matrix. The reward is only provided to the robot after an action was made. In Q-learning, a similar matrix Q is used to record the experience which has been learned by the robot after taking actions. Similar to the reward matrix, the row of the Q matrix records states, while the column records Q values after an action was made.\n",
    "\n",
    "The Q matrix is initialzed to zero as the agent knows nothing about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q0.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q matrix is updated after every action was made according to the following formula:\n",
    "$Q(state,action)=R(state,action)+\\gamma * \\max\\{Q(next state 1, action 1), Q(next state 1, action 2),\\cdots, Q(next state 2, action 1), Q(next state 2, action 2),\\cdots\\}$\n",
    "\n",
    "where $0\\leq\\gamma\\leq 1$ is a numerical value and the values in $\\max\\{\\}$ are all the possible Q values for all the actions in the next state.\n",
    "\n",
    "We can see that our agent can only learn from experience without any teachers. The agent will reach the goal only by exploring the possible states. In Q-learning, each exploration is called an episode. Equivalently, the agent moving from initial state to the goal is called an episode. Q-learning is to perform all these episode until it converges.\n",
    "\n",
    "The following is the Q-learning algorithm:\n",
    "1. Set the environment reward matrix R and set $\\gamma$.\n",
    "2. Initialize Q matrix to zero\n",
    "3. For each episode:<br>\n",
    "    3.1 Select a random initial state <br>\n",
    "    3.2 Do the following steps, while the goal state has not been reached <br>\n",
    "      - Select one among all possible actions for the current state\n",
    "      - Using this possible action, consider going to the next state\n",
    "      - Get maximum Q value for this next state based on all possible actions\n",
    "      - Compute: $Q(state,action)=R(state,action)+\\gamma * \\max\\{Q(next state, action 1), Q(next state, action 2),\\cdots\\}$\n",
    "      - Set the next state as the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent uses this algorithm to learn environment form experience, where each episode can be seen as a training session. The reward matrix R can considered as the environment that the agent explores, while the matrix Q is seen as the knowledge/brain that the agent has learned from the environment.\n",
    "\n",
    "When $\\gamma=0$, the agent will only consider the rewards as knowledge, while when $\\gamma$ approaches $1$, the agent considers more about future reward and less weight on the immediate reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Q matrix is learned, the agent can use Q matrix to trace from start to goal nodes by following the highest reward values in Q matrix, which can be summarized by the following steps:\n",
    "1. Set current state to the initial state\n",
    "2. Given the current state, find the action with highest Q avalue\n",
    "3. Set current state to the next state obtained from step 2\n",
    "4. Repeat steps 2 and 3 untile the current state is the goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An episode example\n",
    "For understanding Q-learning, the following shows an episode training by hand which would provide a taste how Q-learning works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Q to zero\n",
    "\n",
    "Set $\\gamma=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1: suppose the node B was randomly selected. current state = B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2: Do the following steps, while the goal state has not been reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while iteration 1: \n",
    "    - There are three possible actions: B->E, B->C, and B->A. By random selection, B->E was the chosen action.\n",
    "    - The next state = E due the chosen action.\n",
    "    - Based on the next state E, all the possible actions are E->F and E->B. Since Q(E,E->F)=0 and Q(E,E->B)=0, the maximum Q value is 0.\n",
    "    - So Q(B, B->E) = R(B, B->E) + $\\gamma$ * $\\max \\{Q(E,E->F),Q(E,E->B)\\}$ = 0 + 0.5 * 0 = 0\n",
    "    - current state = E\n",
    "    \n",
    "After this iteration the Q matrix does not change as Q(B, B->E) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the current state is the node E <br>\n",
    "while iteration 2: \n",
    "    - There are two possible actions: E-F, and E->B. By random selection, E->F was the chosen action.\n",
    "    - The next state = F due to the chosen action.\n",
    "    - Based on the next state F, all the possible actions are F->G, F->F and F->E. Since Q(F,F->G)=0, Q(F,F->E)=0 and Q(F,F->F)=0, the maximum Q value is 0.\n",
    "    - So $ Q(E, E->F) = R(E, E->F) + \\gamma * \\max \\{Q(F,F->G),Q(F,F->E),Q(F,F->F)\\} = 100 + 0.5 * 0 = 100 $\n",
    "    - current state = F\n",
    "    \n",
    "After this iteration the current state is F. Because this is the goal node, the current episode is finished. The Q matrix now is changed to the following matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat these episode until it converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: write a Python programme to implement this Q-learning algorithm and apply to this example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
