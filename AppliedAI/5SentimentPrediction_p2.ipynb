{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Aberdeen\n",
    "\n",
    "## Applied AI (CS5079)\n",
    "\n",
    "### Lecture (Day 5) - Investigating Sentiment Prediction\n",
    "\n",
    "---\n",
    "\n",
    "In the lecture, we cover tools for pre-processing text data, several supervised/unsupervised models for sentiment prediction and model causation.  This lecture is inspired by Chapter 7 of __Practical Machine Learning with Python__ (2018), Sarkar et al.\n",
    "\n",
    "__In this particular notebook, we study supervised models based on SGDC Classifiers and Logistic Regressions with BOW or TF-IDF features__.\n",
    "\n",
    "We will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual data representation and manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Libraries for feature engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing BOW and TF-IDF features\n",
    "\n",
    "Since Codio only offers 500MB of RAM, we will restrict our dataset to the first 10,000 reviews (instead of the 50,000 reviews). The dataset will then be split into a training and test dataset containing 70% and 30% of the restricted dataset respectively.\n",
    "\n",
    "Feel free to try with the full set of reviews on your personal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_movie_reviews = pd.read_csv(\"Datasets/normalized_movie_reviews.csv\")\n",
    "numberOfReviews=10000\n",
    "reviews = np.array(normalized_movie_reviews['review'].iloc[:numberOfReviews])\n",
    "sentiments = np.array(normalized_movie_reviews['sentiment'].iloc[:numberOfReviews])\n",
    "\n",
    "# extract data for model evaluation\n",
    "train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(reviews, sentiments, test_size=0.3)\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(test_reviews)\n",
    "tv_test_features = tv.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (7000, 581132)  Test features shape: (3000, 581132)\n",
      "TFIDF model:> Train features shape: (7000, 581132)  Test features shape: (3000, 581132)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our SVM and LR models\n",
    "lr = LogisticRegression(penalty='l2', max_iter=1000, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8806666666666667\n",
      "The model precision score is: 0.8807741589652909\n",
      "The model recall score is: 0.8806666666666667\n",
      "The model F1-score is: 0.8806436387571166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      1483\n",
      "    positive       0.88      0.89      0.88      1517\n",
      "\n",
      "    accuracy                           0.88      3000\n",
      "   macro avg       0.88      0.88      0.88      3000\n",
      "weighted avg       0.88      0.88      0.88      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>1290</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>165</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            1290             193\n",
       "Act. positive             165            1352"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression model on BOW features\n",
    "lr.fit(cv_train_features,train_sentiments)\n",
    "y_predicted = lr.predict(cv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(test_sentiments, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(test_sentiments, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(test_sentiments, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8743333333333333\n",
      "The model precision score is: 0.874331981981982\n",
      "The model recall score is: 0.8743333333333333\n",
      "The model F1-score is: 0.8743317832086531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87      1483\n",
      "    positive       0.88      0.88      0.88      1517\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.87      0.87      0.87      3000\n",
      "weighted avg       0.87      0.87      0.87      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>1293</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>187</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            1293             190\n",
       "Act. positive             187            1330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr.fit(tv_train_features,train_sentiments)\n",
    "y_predicted = lr.predict(tv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(test_sentiments, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(test_sentiments, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(test_sentiments, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8713333333333333\n",
      "The model precision score is: 0.8718705534123772\n",
      "The model recall score is: 0.8713333333333333\n",
      "The model F1-score is: 0.8712526229825935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.85      0.87      1483\n",
      "    positive       0.86      0.89      0.88      1517\n",
      "\n",
      "    accuracy                           0.87      3000\n",
      "   macro avg       0.87      0.87      0.87      3000\n",
      "weighted avg       0.87      0.87      0.87      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>1260</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>163</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            1260             223\n",
       "Act. positive             163            1354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM model on BOW\n",
    "svm.fit(cv_train_features,train_sentiments)\n",
    "y_predicted = svm.predict(cv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(test_sentiments, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(test_sentiments, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(test_sentiments, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8943333333333333\n",
      "The model precision score is: 0.8944576905807119\n",
      "The model recall score is: 0.8943333333333333\n",
      "The model F1-score is: 0.8943118735362295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      1483\n",
      "    positive       0.89      0.91      0.90      1517\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.89      0.89      0.89      3000\n",
      "weighted avg       0.89      0.89      0.89      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>1310</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>144</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            1310             173\n",
       "Act. positive             144            1373"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SVM model on TF-IDF\n",
    "svm.fit(tv_train_features,train_sentiments)\n",
    "y_predicted = svm.predict(tv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(test_sentiments, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(test_sentiments, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(test_sentiments, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(test_sentiments, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
